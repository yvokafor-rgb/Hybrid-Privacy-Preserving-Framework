{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -U \"flwr[simulation]\" torch==2.8.0 opacus matplotlib"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Okmy-ySTqISW",
        "outputId": "a6241957-03f1-4cd7-a926-fb5b947523ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.8.0\n",
            "  Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (30 kB)\n",
            "Collecting opacus\n",
            "  Downloading opacus-1.5.4-py3-none-any.whl.metadata (8.7 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Collecting matplotlib\n",
            "  Downloading matplotlib-3.10.7-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (11 kB)\n",
            "Collecting flwr[simulation]\n",
            "  Downloading flwr-1.24.0-py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (2025.3.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.8.93 (from torch==2.8.0)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.8.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.8.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (9.10.2.21)\n",
            "Collecting nvidia-cublas-cu12==12.8.4.1 (from torch==2.8.0)\n",
            "  Downloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.3.83 (from torch==2.8.0)\n",
            "  Downloading nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.9.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.3.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.8.93 (from torch==2.8.0)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.8.0) (0.7.1)\n",
            "Collecting nvidia-nccl-cu12==2.27.3 (from torch==2.8.0)\n",
            "  Downloading nvidia_nccl_cu12-2.27.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.8.90 (from torch==2.8.0)\n",
            "  Downloading nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.8 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.8.93 (from torch==2.8.0)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting nvidia-cufile-cu12==1.13.1.3 (from torch==2.8.0)\n",
            "  Downloading nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting triton==3.4.0 (from torch==2.8.0)\n",
            "  Downloading triton-3.4.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.7 kB)\n",
            "Collecting click<8.2.0 (from flwr[simulation])\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting cryptography<45.0.0,>=44.0.1 (from flwr[simulation])\n",
            "  Downloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.70.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.76.0)\n",
            "Collecting grpcio-health-checking<2.0.0,>=1.70.0 (from flwr[simulation])\n",
            "  Downloading grpcio_health_checking-1.76.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr[simulation])\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.0.2)\n",
            "Collecting pathspec<0.13.0,>=0.12.1 (from flwr[simulation])\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: protobuf<7.0.0,>=5.28.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (5.29.5)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr[simulation])\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (6.0.3)\n",
            "Collecting ray==2.51.1 (from flwr[simulation])\n",
            "  Downloading ray-2.51.1-cp312-cp312-manylinux2014_x86_64.whl.metadata (21 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.32.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Collecting tomli<3.0.0,>=2.0.1 (from flwr[simulation])\n",
            "  Downloading tomli-2.3.0-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (10 kB)\n",
            "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr[simulation])\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: typer<0.21.0,>=0.12.5 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (0.20.0)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray==2.51.1->flwr[simulation]) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray==2.51.1->flwr[simulation]) (1.1.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray==2.51.1->flwr[simulation]) (25.0)\n",
            "Requirement already satisfied: scipy>=1.2 in /usr/local/lib/python3.12/dist-packages (from opacus) (1.16.3)\n",
            "Requirement already satisfied: opt-einsum>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from opacus) (3.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.61.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.0.0)\n",
            "Collecting protobuf<7.0.0,>=5.28.0 (from flwr[simulation])\n",
            "  Downloading protobuf-6.33.2-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.11.12)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch==2.8.0) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.21.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.8.0) (3.0.3)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.23)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.51.1->flwr[simulation]) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.51.1->flwr[simulation]) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.51.1->flwr[simulation]) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.51.1->flwr[simulation]) (0.30.0)\n",
            "Downloading torch-2.8.0-cp312-cp312-manylinux_2_28_x86_64.whl (887.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m887.9/887.9 MB\u001b[0m \u001b[31m788.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (594.3 MB)\n",
            "\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m594.3/594.3 MB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "NgL-pMRgoikl"
      },
      "outputs": [],
      "source": [
        "# =========================\n",
        "# Federated Healthcare (Colab)\n",
        "# Flower (simulation) + PyTorch\n",
        "# =========================\n",
        "\n",
        "import json, math, random, warnings\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_recall_fscore_support, roc_auc_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "import flwr as fl\n",
        "from flwr.common import (\n",
        "    ndarrays_to_parameters,\n",
        "    parameters_to_ndarrays,\n",
        "    NDArrays,\n",
        "    Scalar,\n",
        ")\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# -----------------------------\n",
        "# Config & Utilities\n",
        "# -----------------------------\n",
        "\n",
        "@dataclass\n",
        "class FLConfig:\n",
        "    num_clients: int = 8\n",
        "    num_rounds: int = 5\n",
        "    local_epochs: int = 1\n",
        "    batch_size: int = 32\n",
        "    lr: float = 1e-3\n",
        "    seed: int = 42\n",
        "    dirichlet_alpha: float = 0.5  # client heterogeneity\n",
        "    dp_on: bool = False           # optional DP\n",
        "    dp_noise_multiplier: float = 1.0\n",
        "    dp_max_grad_norm: float = 1.0\n",
        "\n",
        "\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "\n",
        "def is_binary_labels(y: np.ndarray) -> bool:\n",
        "    return len(np.unique(y)) == 2\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Load & preprocess\n",
        "# -----------------------------\n",
        "\n",
        "def engineer_features(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Create useful numeric features from dates and amounts.\"\"\"\n",
        "    df = df.copy()\n",
        "\n",
        "    # Length of stay (days) if dates exist\n",
        "    if \"Date of Admission\" in df.columns and \"Discharge Date\" in df.columns:\n",
        "        adm = pd.to_datetime(df[\"Date of Admission\"], errors=\"coerce\")\n",
        "        dis = pd.to_datetime(df[\"Discharge Date\"], errors=\"coerce\")\n",
        "        df[\"length_of_stay_days\"] = (dis - adm).dt.days\n",
        "\n",
        "    # Clean billing\n",
        "    if \"Billing Amount\" in df.columns:\n",
        "        df[\"Billing Amount\"] = pd.to_numeric(df[\"Billing Amount\"], errors=\"coerce\")\n",
        "\n",
        "    # Normalize casing for certain categoricals\n",
        "    for col in [\"Gender\", \"Blood Type\", \"Medical Condition\", \"Admission Type\",\n",
        "                \"Medication\", \"Insurance Provider\"]:\n",
        "        if col in df.columns and df[col].dtype == object:\n",
        "            df[col] = df[col].astype(str).str.strip().str.title()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def load_healthcare(csv_path: str, target_col: str = \"stroke\"):\n",
        "    \"\"\"\n",
        "    Loads stroke dataset, drops obvious ID/PII, preprocesses features,\n",
        "    and returns train/test splits suitable for FL.\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    # If the stroke column is not present, raise an error\n",
        "    if target_col not in df.columns:\n",
        "        raise ValueError(f\"Target column '{target_col}' not found. Available: {list(df.columns)}\")\n",
        "\n",
        "    # Drop obvious non-predictive identifiers if they exist\n",
        "    drop_cols = [c for c in [\"id\", \"Name\", \"Doctor\", \"Hospital\"] if c in df.columns]\n",
        "    df = df.drop(columns=drop_cols, errors=\"ignore\")\n",
        "\n",
        "    # Feature engineering (won't do much here but safe to keep)\n",
        "    df = engineer_features(df)\n",
        "\n",
        "    # Target: stroke (0/1)\n",
        "    y_raw = df[target_col]\n",
        "    # Force to numeric 0/1\n",
        "    y = pd.to_numeric(y_raw, errors=\"coerce\").astype(float)\n",
        "    # Drop rows where target is NaN after conversion\n",
        "    mask = ~np.isnan(y)\n",
        "    df = df.loc[mask].reset_index(drop=True)\n",
        "    y = y[mask].astype(int).values\n",
        "\n",
        "    # Features: all columns except target\n",
        "    X = df.drop(columns=[target_col])\n",
        "\n",
        "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
        "    categorical_cols = [c for c in X.columns if c not in numeric_cols]\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", Pipeline([\n",
        "                (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "                (\"scaler\", StandardScaler())\n",
        "            ]), numeric_cols),\n",
        "            (\"cat\", Pipeline([\n",
        "                (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False))\n",
        "            ]), categorical_cols),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    strat = y if is_binary_labels(y) else None\n",
        "    X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=strat\n",
        "    )\n",
        "\n",
        "    X_train = preprocessor.fit_transform(X_train_raw)\n",
        "    X_test = preprocessor.transform(X_test_raw)\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test = np.array(X_test)\n",
        "    y_train = np.array(y_train)\n",
        "    y_test = np.array(y_test)\n",
        "\n",
        "    return X_train, X_test, y_train, y_test, preprocessor\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Federated partition (non-IID)\n",
        "# -----------------------------\n",
        "\n",
        "def dirichlet_partition(X, y, num_clients: int, alpha: float = 0.5, seed: int = 42):\n",
        "    \"\"\"Non-IID Dirichlet partition of data into num_clients splits.\"\"\"\n",
        "    rng = np.random.default_rng(seed)\n",
        "    classes = np.unique(y)\n",
        "    idx_by_class = {c: np.where(y == c)[0] for c in classes}\n",
        "    client_indices = [[] for _ in range(num_clients)]\n",
        "\n",
        "    for c in classes:\n",
        "        idxs = idx_by_class[c]\n",
        "        rng.shuffle(idxs)\n",
        "        props = rng.dirichlet(alpha=[alpha] * num_clients)\n",
        "        counts = np.floor(props * len(idxs)).astype(int)\n",
        "        while counts.sum() < len(idxs):\n",
        "            counts[rng.integers(0, num_clients)] += 1\n",
        "        start = 0\n",
        "        for i in range(num_clients):\n",
        "            end = start + counts[i]\n",
        "            client_indices[i].extend(idxs[start:end].tolist())\n",
        "            start = end\n",
        "\n",
        "    splits = []\n",
        "    X = np.array(X)\n",
        "    y = np.array(y)\n",
        "    for ci in client_indices:\n",
        "        ci = np.array(ci, dtype=int)\n",
        "        # Only include splits with data\n",
        "        if len(ci) > 0:\n",
        "            splits.append((X[ci], y[ci]))\n",
        "    return splits\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Flatten helpers for parameters\n",
        "# -----------------------------\n",
        "\n",
        "def flatten_ndarrays(nds: List[np.ndarray]) -> Tuple[np.ndarray, List[Tuple[int, ...]]]:\n",
        "    \"\"\"Flatten a list of ndarrays into a single 1D vector + remember shapes.\"\"\"\n",
        "    shapes = [a.shape for a in nds]\n",
        "    flats = [a.ravel() for a in nds]\n",
        "    flat = np.concatenate(flats).astype(np.float64)\n",
        "    return flat, shapes\n",
        "\n",
        "def unflatten_ndarrays(flat: np.ndarray, shapes: List[Tuple[int, ...]]) -> List[np.ndarray]:\n",
        "    \"\"\"Rebuild list of ndarrays from flat vector + shapes.\"\"\"\n",
        "    out = []\n",
        "    i = 0\n",
        "    for s in shapes:\n",
        "        n = int(np.prod(s))\n",
        "        part = flat[i:i+n].reshape(s)\n",
        "        out.append(part)\n",
        "        i += n\n",
        "    return out\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Model & Training helpers\n",
        "# -----------------------------\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, 128), nn.ReLU(),\n",
        "            nn.Linear(128, 64), nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(1)\n",
        "\n",
        "\n",
        "def make_pos_weight(y: np.ndarray):\n",
        "    \"\"\"For BCEWithLogitsLoss: pos_weight = N_neg / N_pos.\"\"\"\n",
        "    unique, counts = np.unique(y, return_counts=True)\n",
        "    cdict = dict(zip(unique, counts))\n",
        "    if 0 in cdict and 1 in cdict and cdict[1] > 0:\n",
        "        return torch.tensor(cdict[0] / cdict[1], dtype=torch.float32)\n",
        "    return torch.tensor(1.0, dtype=torch.float32)\n",
        "\n",
        "\n",
        "def to_tensor_dataset(X: np.ndarray, y: np.ndarray) -> TensorDataset:\n",
        "    return TensorDataset(torch.tensor(X, dtype=torch.float32),\n",
        "                         torch.tensor(y, dtype=torch.float32))\n",
        "\n",
        "\n",
        "def bce_metrics(logits: np.ndarray, y_true: np.ndarray) -> Dict[str, float]:\n",
        "    probs = 1 / (1 + np.exp(-logits))\n",
        "    y_pred = (probs >= 0.5).astype(int)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
        "        y_true, y_pred, average=\"binary\", zero_division=0\n",
        "    )\n",
        "    try:\n",
        "        auc = roc_auc_score(y_true, probs)\n",
        "    except Exception:\n",
        "        auc = float(\"nan\")\n",
        "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1, \"roc_auc\": auc}\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Flower Client\n",
        "# -----------------------------\n",
        "\n",
        "class TabularClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid: int, X: np.ndarray, y: np.ndarray, input_dim: int, cfg: FLConfig):\n",
        "        self.cid = cid\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.cfg = cfg\n",
        "\n",
        "        self.model = MLP(input_dim)\n",
        "\n",
        "        if is_binary_labels(self.y):\n",
        "            self.criterion = nn.BCEWithLogitsLoss(pos_weight=make_pos_weight(self.y))\n",
        "        else:\n",
        "            self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.cfg.lr)\n",
        "\n",
        "    def get_parameters(self, config={}):\n",
        "        return [v.cpu().numpy() for _, v in self.model.state_dict().items()]\n",
        "\n",
        "    def set_parameters(self, params):\n",
        "        state_dict = self.model.state_dict()\n",
        "        for (k, _), v in zip(state_dict.items(), params):\n",
        "            state_dict[k] = torch.tensor(v)\n",
        "        self.model.load_state_dict(state_dict)\n",
        "\n",
        "    def fit(self, params, config={}):\n",
        "        self.set_parameters(params)\n",
        "        # Ensure X is not empty before creating DataLoader\n",
        "        if len(self.X) == 0:\n",
        "            print(f\"Client {self.cid} has no data, skipping fit.\")\n",
        "            return self.get_parameters(), 0, {}\n",
        "\n",
        "        loader = DataLoader(\n",
        "            to_tensor_dataset(self.X, self.y),\n",
        "            batch_size=self.cfg.batch_size,\n",
        "            shuffle=True,\n",
        "        )\n",
        "        self.model.train()\n",
        "        for _ in range(self.cfg.local_epochs):\n",
        "            for xb, yb in loader:\n",
        "                logits = self.model(xb)\n",
        "                loss = self.criterion(logits, yb)\n",
        "                self.optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "        return self.get_parameters(), len(self.X), {}\n",
        "\n",
        "    def evaluate(self, params, config={}):\n",
        "        self.set_parameters(params)\n",
        "        if len(self.X) == 0:\n",
        "            print(f\"Client {self.cid} has no data, skipping evaluation.\")\n",
        "            return 0.0, 0, {\"accuracy\": 0.0, \"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"roc_auc\": float(\"nan\")}\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.model(torch.tensor(self.X, dtype=torch.float32)).cpu().numpy()\n",
        "        m = bce_metrics(logits, self.y)\n",
        "        loss = float(\n",
        "            nn.BCEWithLogitsLoss()(\n",
        "                torch.tensor(logits, dtype=torch.float32),\n",
        "                torch.tensor(self.y, dtype=torch.float32)\n",
        "            ).item()\n",
        "        )\n",
        "        return loss, len(self.X), m\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Server-side (test set) evaluation\n",
        "# -----------------------------\n",
        "\n",
        "def gen_evaluate_fn(X_test: np.ndarray, y_test: np.ndarray, input_dim: int, cfg: FLConfig):\n",
        "    def evaluate(server_round: int, parameters: fl.common.NDArrays, config):\n",
        "        model = MLP(input_dim)\n",
        "        state_dict = model.state_dict()\n",
        "        for (k, _), v in zip(state_dict.items(), parameters):\n",
        "            state_dict[k] = torch.tensor(v)\n",
        "        model.load_state_dict(state_dict)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = model(torch.tensor(X_test, dtype=torch.float32)).cpu().numpy()\n",
        "        metrics = bce_metrics(logits, y_test)\n",
        "\n",
        "        print(f\"[Round {server_round}] test: \" +\n",
        "              json.dumps({k: round(v, 4) if v == v else None for k, v in metrics.items()}))\n",
        "\n",
        "        loss = float(\n",
        "            nn.BCEWithLogitsLoss()(\n",
        "                torch.tensor(logits, dtype=torch.float32),\n",
        "                torch.tensor(y_test, dtype=torch.float32)\n",
        "            ).item()\n",
        "        )\n",
        "        return loss, metrics\n",
        "    return evaluate\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Custom FedAvg that logs updates for HE notebook\n",
        "# -----------------------------\n",
        "\n",
        "class LoggingFedAvg(fl.server.strategy.FedAvg):\n",
        "    \"\"\"\n",
        "    Same as FedAvg, but on a chosen round it logs client deltas Î”w_i\n",
        "    (local_i - global_after_round) to 'round1_updates.npy'\n",
        "    for later use in a separate HE notebook.\n",
        "    \"\"\"\n",
        "    def __init__(self, log_round: int = 1, log_path: str = \"round1_updates.npy\", **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.log_round = log_round\n",
        "        self.log_path = log_path\n",
        "        self._shapes_cache = None\n",
        "        self._logged = False\n",
        "\n",
        "    def aggregate_fit(\n",
        "        self,\n",
        "        rnd: int,\n",
        "        results: List[Tuple[fl.server.client_proxy.ClientProxy, fl.server.client_proxy.FitRes]],\n",
        "        failures: List[BaseException],\n",
        "    ) -> Tuple[NDArrays, Dict[str, Scalar]]:\n",
        "\n",
        "        # 1) Let FedAvg do the usual aggregation\n",
        "        aggregated_params, metrics = super().aggregate_fit(rnd, results, failures)\n",
        "\n",
        "        # 2) Log once, for the chosen round\n",
        "        if (not self._logged) and (rnd == self.log_round) and results:\n",
        "            print(f\"ðŸ“¥ Logging client updates from round {rnd} to '{self.log_path}'\")\n",
        "\n",
        "            # Global AFTER aggregation for this round\n",
        "            global_nd = parameters_to_ndarrays(aggregated_params)\n",
        "            flat_global, shapes = flatten_ndarrays(global_nd)\n",
        "            self._shapes_cache = shapes\n",
        "\n",
        "            updates = []\n",
        "            for _, fitres in results:\n",
        "                local_nd = parameters_to_ndarrays(fitres.parameters)  # client's local model\n",
        "                flat_local, _ = flatten_ndarrays(local_nd)\n",
        "                delta = flat_local - flat_global  # Î”w_i = local_i - global_after\n",
        "                updates.append(delta)\n",
        "\n",
        "            updates = np.stack(updates, axis=0)  # shape: [num_clients, D]\n",
        "\n",
        "            np.save(self.log_path, updates)\n",
        "            np.save(self.log_path.replace(\".npy\", \"_shapes.npy\"),\n",
        "                    np.array(self._shapes_cache, dtype=object))\n",
        "            print(f\"âœ… Saved shape {updates.shape} to '{self.log_path}'\")\n",
        "\n",
        "            self._logged = True\n",
        "\n",
        "        return aggregated_params, metrics\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Orchestration\n",
        "# -----------------------------\n",
        "\n",
        "def run_federated(csv_path: str, target_col: str = \"Test Results\", cfg: FLConfig = FLConfig()):\n",
        "    set_seed(cfg.seed)\n",
        "\n",
        "    # Load & preprocess\n",
        "    X_train, X_test, y_train, y_test, preproc = load_healthcare(csv_path, target_col)\n",
        "    input_dim = X_train.shape[1]\n",
        "\n",
        "    # Non-IID client splits\n",
        "    client_splits = dirichlet_partition(\n",
        "        X_train, y_train, cfg.num_clients,\n",
        "        alpha=cfg.dirichlet_alpha, seed=cfg.seed\n",
        "    )\n",
        "\n",
        "    def client_fn(cid: str):\n",
        "        i = int(cid)\n",
        "        # Ensure client_splits has enough elements\n",
        "        if i < len(client_splits):\n",
        "            Xc, yc = client_splits[i]\n",
        "            return TabularClient(i, Xc, yc, input_dim, cfg)\n",
        "        else:\n",
        "            # Handle cases where client_splits might have fewer clients than cfg.num_clients\n",
        "            # This can happen if some partitions ended up empty and were filtered out.\n",
        "            # For now, we return a dummy client that doesn't train/evaluate.\n",
        "            print(f\"Client {cid} requested, but no data available. Returning a dummy client.\")\n",
        "            return TabularClient(i, np.array([]).reshape(0, input_dim), np.array([]), input_dim, cfg)\n",
        "\n",
        "    strategy = LoggingFedAvg(\n",
        "        log_round=1,                     # log round-1 updates for HE notebook\n",
        "        log_path=\"round1_updates.npy\",\n",
        "        evaluate_fn=gen_evaluate_fn(X_test, y_test, input_dim, cfg),\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=1.0,\n",
        "        min_fit_clients=cfg.num_clients, # Changed this to allow simulation to proceed with fewer actual clients\n",
        "        min_evaluate_clients=cfg.num_clients,\n",
        "        min_available_clients=cfg.num_clients,\n",
        "    )\n",
        "\n",
        "    hist = fl.simulation.start_simulation(\n",
        "        client_fn=client_fn,\n",
        "        num_clients=cfg.num_clients,\n",
        "        config=fl.server.ServerConfig(num_rounds=cfg.num_rounds),\n",
        "        strategy=strategy,\n",
        "    )\n",
        "\n",
        "    print(\"\\nâœ… Training completed successfully.\")\n",
        "    return hist, X_train, X_test, y_test\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Run\n",
        "# -----------------------------\n",
        "\n",
        "hist, X_train, X_test, y_test = run_federated(\n",
        "    \"healthcare-dataset-stroke-data.csv\",\n",
        "    target_col=\"stroke\",\n",
        "    cfg=FLConfig(num_clients=8, num_rounds=5, local_epochs=1, batch_size=32, lr=1e-3),\n",
        ")\n",
        "\n",
        "print(\"âœ… Training complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5832d949"
      },
      "source": [
        "display(hist.metrics_centralized)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Load updates saved by FL\n",
        "import numpy as np\n",
        "updates = np.load(\"/content/round1_updates.npy\")\n",
        "print(\"Loaded updates from FL:\", updates.shape)\n",
        "\n",
        "# 3. Blockchain import\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive')\n",
        "from mock_ledger import MockBlockchain\n",
        "\n",
        "ledger = MockBlockchain()\n",
        "\n",
        "# Store each client update\n",
        "for i in range(updates.shape[0]):\n",
        "    payload = updates[i].tobytes()\n",
        "    ledger.submit_update(1, f\"client_{i}\", payload)\n",
        "\n",
        "# 4. Save ledger locally\n",
        "import pickle\n",
        "with open(\"ledger.pkl\", \"wb\") as f:\n",
        "    pickle.dump(ledger, f)\n",
        "\n",
        "print(\"Saved ledger to local file 'ledger.pkl'\")\n",
        "\n",
        "#Copy all files into Drive\n",
        "!cp round1_updates.npy /content/drive/MyDrive/\n",
        "!cp round1_updates_shapes.npy /content/drive/MyDrive/\n",
        "!cp ledger.pkl /content/drive/MyDrive/\n",
        "\n",
        "print(\" All files copied into Google Drive\")\n"
      ],
      "metadata": {
        "id": "MZFfHmirdJcH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}